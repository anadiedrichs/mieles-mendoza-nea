---
title: "Dataset"
author: "Ana Diedrichs"
date: "10/29/2020"
output: 
  html_document: 
    toc: TRUE 
    

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
#library(DataExplorer)
```

# Dataset 

## Carga y limpieza

Cargar el dataset 

```{r}
library(readxl)
mieles_mza_nea <- read_excel("data/mieles-mza-nea.xls", 
                             n_max = 205)

```

Limpiamos las columnas que no necesitamos. Renombramos 208 Pb como Pb

```{r}
dataset <- 
  mieles_mza_nea %>%  
  select(-starts_with("Columna")) %>% 
  rename( "Pb" = "208 Pb")
```

## Variables

En la columna Origin tenemos los orígenes de las muestras

```{r}
unique(dataset$Origin)

```

* *VU* Valle de Uco
* *Sur* , Sur de Mendoza, departamentos San Rafael, Malargue y  ???? (Ver Brenda)

# EDA 

## Correlation plot

```{r}
#plot_correlation(dataset)

library(corrr)

dataset %>% select(-c(Origin) ) %>%  correlate()

dataset %>% select(-c(Origin) ) %>%  correlate() %>% rplot()


```

## Plot boxplot según Origin

```{r}

#plot_boxplot(dataset,by="Origin")
```


```{r}
dataset %>%  pivot_longer(cols = -c(Origin)) %>% 
  ggplot(aes(value,name))+
  geom_boxplot() +
  facet_grid(~Origin)
```


```{r}
dataset %>%  pivot_longer(cols = -c(Origin)) %>% 
  ggplot(aes(value,name))+
  geom_boxplot() +
  facet_wrap(~Origin)
```


```{r}
dataset %>%  
  select(Na,Origin,Mn,Mg,Al,Cr) %>% 
  pivot_longer(cols = -c(Origin)) %>% 
  ggplot(aes(value,name))+
  geom_boxplot() +
  facet_wrap(~Origin)
```

# Modelando con tidymodels


Métricas a usar en general 

```{r}

metrics <- metric_set(kap, accuracy, sensitivity, specificity)
```

## Desde el trabajo de Diana

Usar Leave-one-out y LDA no es posible, da error y el libro tmwr dice


Así que será cross validation con LDA

### LDA con cross-validation 

```{r}
# cross validation 
library(tidymodels)
library(discrim)
# train y test datasets 
set.seed(123)

diana_dataset <- dataset %>% filter(Origin %in% c("CHA","CTE","MNE","FSA"))

d_split <- initial_split(diana_dataset, strata = Origin) # 75% train 25% test
d_train <- training(d_split)
d_test <- testing(d_split)

set.seed(123)
d_folds <- vfold_cv(d_train,strata= "Origin") # 5 folds
d_folds

d_lda <- discrim_linear() %>% 
  set_engine("MASS")


d_receta <- recipe(Origin ~ ., data = d_train) 

wf <- workflow() %>%
  add_recipe(d_receta)


lda_rs <- wf %>%
  add_model(d_lda) %>%
  fit_resamples(
    resamples = d_folds,
    metrics = metrics,
    control = control_resamples(save_pred = TRUE)
  )

#lda_rs


collect_metrics(lda_rs)

d_final <- wf %>%
  add_model(d_lda) %>%
  last_fit(d_split)

d_final

collect_metrics(d_final)
```

### Con redes neuronales 


```{r}
# cross validation 
# train y test datasets 
set.seed(123)

diana_dataset <- dataset %>% filter(Origin %in% c("CHA","CTE","MNE","FSA"))

d_split <- initial_split(diana_dataset, strata = Origin) # 75% train 25% test
d_train <- training(d_split)
d_test <- testing(d_split)

set.seed(123)
d_folds <- vfold_cv(d_train,strata= "Origin") # 5 folds
d_folds

d_nnet <-  mlp(hidden_units=30) %>% 
  set_engine("nnet") %>% 
  set_mode("classification")


d_receta <- recipe(Origin ~ ., data = d_train) 

wf <- workflow() %>%
  add_recipe(d_receta)

metrics <- metric_set(kap, accuracy, sensitivity, specificity)

nnet_rs <- wf %>%
  add_model(d_nnet) %>%
  fit_resamples(
    resamples = d_folds,
    metrics = metrics,
    control = control_resamples(save_pred = TRUE)
  )

#lda_rs


collect_metrics(nnet_rs)

d_final_nnet <- wf %>%
  add_model(d_nnet) %>%
  last_fit(d_split)

#d_final_nnet

collect_metrics(d_final_nnet)
```



### Caso random forest

```{r}
# cross validation 
# train y test datasets 
set.seed(123)

diana_dataset <- dataset %>% filter(Origin %in% c("CHA","CTE","MNE","FSA"))

d_split <- initial_split(diana_dataset, strata = Origin) # 75% train 25% test
d_train <- training(d_split)
d_test <- testing(d_split)

set.seed(123)
d_folds <- vfold_cv(d_train,strata= "Origin") # 5 folds
d_folds

d_rf <- rand_forest() %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

d_receta <- recipe(Origin ~ ., data = d_train) 

wf <- workflow() %>%
  add_recipe(d_receta)


rf_rs <- wf %>%
  add_model(d_rf) %>%
  fit_resamples(
    resamples = d_folds,
    metrics = metrics,
    control = control_resamples(save_pred = TRUE)
  )

#rf_rs


collect_metrics(rf_rs)

d_final_rf <- wf %>%
  add_model(d_rf) %>%
  last_fit(d_split)

#d_final

collect_metrics(d_final_rf)
```


### Conclusiones

Los resultados para cada modelo, con 5-CV, son los siguientes:

LDA 

```{r}


collect_metrics(d_final)
```
Red neuronal
```{r}

collect_metrics(d_final_nnet)
```
Random forest
```{r}

collect_metrics(d_final_rf)
```

Sensibilidad baja en el training y testing.

Sin lugar a dudas el 0.76 de accuracy de Diana, no contempla el gran desbalance de las clases, es decir, 
no es comparable con al sensibilidad. 

Se ven resultados prometedores con random forest. Todo esto, sin haber balanceado el dataset.


# Mètricas en tidymodels

Accuracy and Kappa use the same definitions as their binary counterpart, with accuracy counting up the number of correctly predicted true values out of the total number of true values, and kappa being a linear combination of two accuracy values.

Matthews correlation coefficient (MCC) has a known multiclass generalization as well, sometimes called the RK statistic. Refer to this page for more details.

ROC AUC is an interesting metric in that it intuitively makes sense to perform macro averaging, which computes a multiclass AUC as the average of the area under multiple binary ROC curves. However, this loses an important property of the ROC AUC statistic in that its binary case is insensitive to class distribution. To combat this, a multiclass metric was created that retains insensitivity to class distribution, but does not have an easy visual interpretation like macro averaging. This is implemented as the "hand_till" method, and is the default for this metric.

Ese texto anterior es de la documentaciòn, ver enlace https://yardstick.tidymodels.org/articles/multiclass.html


# Caso de nuestro interés

## Train y test 

En primer lugar dividimos, armamos la receta, para dividir dataset de entrenamiento  de testeo

```{r}
library(tidymodels)
library(themis)
# train y test datasets 
set.seed(123)
split <- initial_split(dataset, strata = Origin) # 75% train 25% test
train <- training(split)
test <- testing(split)


```

## Cross-validation 10 folds 


```{r}
# cross validation 

set.seed(123)
folds <- vfold_cv(train, strata = Origin, v= 5) # 5 folds
folds
```

Modelo base de random forest 

```{r}
rf_spec <- rand_forest(trees = 500) %>%
  set_mode("classification") %>%
  set_engine("ranger")

rf_spec
```


## Primer intento, uso de SMOTE para balanceo de las siete clases 


```{r}

receta <- recipe(Origin ~ ., data = train) %>%
   step_smote(Origin, over_ratio= 1) 

wf <- workflow() %>%
  add_recipe(receta)


rf_rs <- wf %>%
  add_model(rf_spec) %>%
  fit_resamples(
    resamples = folds,
    metrics = metrics,
    control = control_resamples(save_pred = TRUE)
  )

rf_rs


collect_metrics(rf_rs)




```

```{r}

matriz <- rf_rs %>%
  conf_mat_resampled()


matriz %>% filter(Prediction == "Noreste")
```


```{r}

final <- wf %>%
  add_model(rf_spec) %>%
  last_fit(split)

final

collect_metrics(final)
```


### Conclusiones de lo anterior 

```{r}

receta_sin_smote <- recipe(Origin ~ ., data = train) 

wf_sin_smote <- workflow() %>%
  add_recipe(receta_sin_smote)


rf_sin_smote <- wf_sin_smote %>%
  add_model(rf_spec) %>%
  fit_resamples(
    resamples = folds,
    metrics = metrics,
    control = control_resamples(save_pred = TRUE)
  )

rf_sin_smote


collect_metrics(rf_sin_smote)

```


```{r, eval = FALSE }

matriz <- rf_sin_smote %>%
  conf_mat_resampled()


matriz %>% filter(Prediction == "Noreste")


final <- wf_sin_smote %>%
  add_model(rf_sin_smote) %>%
  last_fit(split)

final

collect_metrics(final)
```


### Conclusiones


Evidentemente mejora la clasificación multiclase al usar SMOTe vs no usarlo. Esto nos da la intuicion de trabajar con desbalanceo de clases en el problema.


## Convirtiendo el problema a clases binarias, clasificador binario

Mendoza vs Norte argentino

```{r}
dataset %>%  select( Origin) %>% ggplot(aes(Origin)) + geom_histogram(stat="count")
```




```{r}
filtro_norte <- c("CHA","CTE","FSA","MNE")
filtro_mza <- c("Noreste","Sur","VU")

data_bin <- dataset 

#data_bin[which(data_bin$Origin %in% c(filtro_norte,"Noreste","Sur")),"Origin"] <- "Norte"

data_bin[which(data_bin$Origin %in% filtro_norte),"Origin"] <- "Norte"

#data_bin[which(data_bin$Origin %in% "VU"),"Origin"] <- "Mza"

data_bin[which(data_bin$Origin %in% filtro_mza),"Origin"] <- "Mza"



```


```{r}
#plot_boxplot(data_bin, by="Origin")
```


Correr modelo binario con SMOTE 

```{r}
set.seed(123)

split <- initial_split(data_bin, strata = Origin) # 75% train 25% test
train <- training(split)
test <- testing(split)


receta_binaria <- recipe(Origin ~ ., data = train) %>%
   step_smote(Origin, over_ratio= 1) 

wf_bin <- workflow() %>%
  add_recipe(receta_binaria)


rf_bin <- wf %>%
  add_model(rf_spec) %>%
  fit_resamples(
    resamples = folds,
    metrics = metrics,
    control = control_resamples(save_pred = TRUE)
  )

rf_bin


collect_metrics(rf_bin)




```



```{r}

matriz <- rf_bin %>%
  conf_mat_resampled()


matriz %>% filter(Prediction == "Noreste")


final <- wf_bin %>%
  add_model(rf_spec) %>%
  last_fit(split)

final

collect_metrics(final)
```

### Con ROSE 

DA ERRROR !!! REVISAR LUEGO !!! 


```{r, eval= FALSE}
set.seed(123)

split <- initial_split(data_bin, strata = Origin) # 75% train 25% test
train <- training(split)
test <- testing(split)


receta_binaria <- recipe(Origin ~ ., data = train) %>%
   step_rose(Origin) 

wf_bin <- workflow() %>%
  add_recipe(receta_binaria)

metrics <- metric_set(kap, accuracy, sensitivity, specificity)

rf_bin <- wf %>%
  add_model(rf_spec) %>%
  fit_resamples(
    resamples = folds,
    metrics = metrics,
    control = control_resamples(save_pred = TRUE)
  )

rf_bin


collect_metrics(rf_bin)




```



```{r}

matriz <- rf_bin %>%
  conf_mat_resampled()


matriz %>% filter(Prediction == "Noreste")


final <- wf_bin %>%
  add_model(rf_spec) %>%
  last_fit(split)

final

collect_metrics(final)
```

## Mejora el modelo con preprocesamiento UMAP o PCA ?

# Conclusiones generales 

Con SMOTE mejora significativamnete el desempeño de los clasificadores.
En el caso del clasificador binario:
* VU vs todos es muy bueno el resultado
* mUY buen resultado en mendoza vs norte 

# TODO list

* Revisar el paso a paso de Tidymodels 







